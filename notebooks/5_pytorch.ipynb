{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook predicts the `beer_style` using a neural network on the PyTorch\n",
    "framework. It is a modification of the 3_pytorch.ipynb notebook. Previous\n",
    "results have indicated that the model might not be complex enough, i.e. the\n",
    "validation and test accuracies are similar and low. This is also a reasonable\n",
    "hypothesis because there are many rows with a very smaller number\n",
    "of features.\n",
    "\n",
    "This notebook will increase the number of neurons as well as add another layer.\n",
    "\n",
    "## Summary\n",
    "Reducing the number of unique `brewery_names` did not lead to a model with\n",
    "better performance. The [classification report](#Classification-report) shows\n",
    "that the test accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "artefact_prefix = '5_pytorch'\n",
    "target = 'beer_style'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from dotenv import find_dotenv\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from category_encoders.binary import BinaryEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from joblib import dump, load\n",
    "\n",
    "from src.data.sets import merge_categories\n",
    "from src.data.sets import save_sets\n",
    "from src.data.sets import load_sets\n",
    "from src.data.sets import split_sets_random\n",
    "from src.data.sets import test_class_exclusion\n",
    "from src.models.performance import convert_cr_to_dataframe\n",
    "from src.models.pytorch import PytorchClassification_5\n",
    "from src.models.pytorch import get_device\n",
    "from src.models.pytorch import train_classification\n",
    "from src.models.pytorch import test_classification\n",
    "from src.models.pytorch import PytorchDataset\n",
    "from src.models.pipes import create_preprocessing_pipe\n",
    "from src.visualization.visualize import plot_confusion_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Set up directories"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "project_dir = Path(find_dotenv()).parent\n",
    "data_dir = project_dir / 'data'\n",
    "raw_data_dir = data_dir / 'raw'\n",
    "interim_data_dir = data_dir / 'interim'\n",
    "processed_data_dir = data_dir / 'processed'\n",
    "reports_dir = project_dir / 'reports'\n",
    "models_dir = project_dir / 'models'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Set directory\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "X_train, X_test, X_val, y_train, y_test, y_val = load_sets()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocess data\n",
    "\n",
    "1. The `brewery_name` is a feature with a very high cardinality, ~5700. One hot encoding is not feasible as it will introduce 5700 very sparse columns. Another option is to use binary encoding, which would result in 14 new columns.\n",
    "1. Standard scaling is used to ensure that the binary columns ([0, 1])and the review columns ([1, 5]) are on the same scale."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('bin_encoder', BinaryEncoder(cols=['brewery_name'])),\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "X_train_trans = pipe.fit_transform(X_train)\n",
    "X_val_trans = pipe.transform(X_val)\n",
    "X_test_trans = pipe.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(951968, 18)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_trans.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "18"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = X_train_trans.shape[1]\n",
    "n_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "104"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = y_train.nunique()\n",
    "n_classes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Encoding\n",
    "\n",
    "PyTorch accepts only numerical labels."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Roger\\.conda\\envs\\adsi_ass_2\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:251: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Roger\\.conda\\envs\\adsi_ass_2\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_train_trans = le.fit_transform(y_train.to_frame())\n",
    "y_val_trans = le.fit_transform(y_val.to_frame())\n",
    "y_test_trans = le.transform(y_test.to_frame())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "array([98, 89,  2, ..., 37, 94, 98])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_trans"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convert to Pytorch tensors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda', index=0)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_device()\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "train_dataset = PytorchDataset(X=X_train_trans, y=y_train_trans)\n",
    "val_dataset = PytorchDataset(X=X_val_trans, y=y_val_trans)\n",
    "test_dataset = PytorchDataset(X=X_test_trans, y=y_test_trans)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classification model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "PytorchClassification_5(\n  (layer_1): Linear(in_features=18, out_features=1024, bias=True)\n  (batchnorm1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layer_2): Linear(in_features=1024, out_features=256, bias=True)\n  (batchnorm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layer_3): Linear(in_features=256, out_features=128, bias=True)\n  (batchnorm3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layer_4): Linear(in_features=128, out_features=64, bias=True)\n  (batchnorm4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layer_out): Linear(in_features=64, out_features=104, bias=True)\n  (relu): ReLU()\n  (dropout): Dropout(p=0.2, inplace=False)\n)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PytorchClassification_5(n_features=n_features, n_classes=n_classes)\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "PytorchClassification_5(\n  (layer_1): Linear(in_features=18, out_features=1024, bias=True)\n  (batchnorm1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layer_2): Linear(in_features=1024, out_features=256, bias=True)\n  (batchnorm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layer_3): Linear(in_features=256, out_features=128, bias=True)\n  (batchnorm3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layer_4): Linear(in_features=128, out_features=64, bias=True)\n  (batchnorm4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layer_out): Linear(in_features=64, out_features=104, bias=True)\n  (relu): ReLU()\n  (dropout): Dropout(p=0.2, inplace=False)\n)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "N_EPOCHS = 20\n",
    "BATCH_SIZE = 1024\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started: 2021-03-12 20:59:13.507343\n",
      "Epoch: 0\n",
      "\t(train)\tLoss: 0.0032\t|\tAcc: 18.8%\n",
      "\t(valid)\tLoss: 0.0028\t|\tAcc: 25.8%\n",
      "Epoch: 1\n",
      "\t(train)\tLoss: 0.0029\t|\tAcc: 23.7%\n",
      "\t(valid)\tLoss: 0.0026\t|\tAcc: 27.2%\n",
      "Epoch: 2\n",
      "\t(train)\tLoss: 0.0028\t|\tAcc: 24.8%\n",
      "\t(valid)\tLoss: 0.0026\t|\tAcc: 27.9%\n",
      "Epoch: 3\n",
      "\t(train)\tLoss: 0.0027\t|\tAcc: 25.4%\n",
      "\t(valid)\tLoss: 0.0025\t|\tAcc: 28.3%\n",
      "Epoch: 4\n",
      "\t(train)\tLoss: 0.0027\t|\tAcc: 25.8%\n",
      "\t(valid)\tLoss: 0.0025\t|\tAcc: 28.5%\n",
      "Epoch: 5\n",
      "\t(train)\tLoss: 0.0027\t|\tAcc: 26.1%\n",
      "\t(valid)\tLoss: 0.0025\t|\tAcc: 29.0%\n",
      "Epoch: 6\n",
      "\t(train)\tLoss: 0.0026\t|\tAcc: 26.4%\n",
      "\t(valid)\tLoss: 0.0025\t|\tAcc: 29.0%\n",
      "Epoch: 7\n",
      "\t(train)\tLoss: 0.0026\t|\tAcc: 26.6%\n",
      "\t(valid)\tLoss: 0.0024\t|\tAcc: 29.3%\n",
      "Epoch: 8\n",
      "\t(train)\tLoss: 0.0026\t|\tAcc: 26.7%\n",
      "\t(valid)\tLoss: 0.0024\t|\tAcc: 29.4%\n",
      "Epoch: 9\n",
      "\t(train)\tLoss: 0.0026\t|\tAcc: 26.8%\n",
      "\t(valid)\tLoss: 0.0024\t|\tAcc: 29.5%\n",
      "Epoch: 10\n",
      "\t(train)\tLoss: 0.0026\t|\tAcc: 26.9%\n",
      "\t(valid)\tLoss: 0.0024\t|\tAcc: 29.5%\n",
      "Epoch: 11\n",
      "\t(train)\tLoss: 0.0026\t|\tAcc: 27.1%\n",
      "\t(valid)\tLoss: 0.0024\t|\tAcc: 29.8%\n",
      "Epoch: 12\n",
      "\t(train)\tLoss: 0.0026\t|\tAcc: 27.2%\n",
      "\t(valid)\tLoss: 0.0024\t|\tAcc: 29.8%\n",
      "Epoch: 13\n",
      "\t(train)\tLoss: 0.0026\t|\tAcc: 27.3%\n",
      "\t(valid)\tLoss: 0.0024\t|\tAcc: 29.8%\n",
      "Epoch: 14\n",
      "\t(train)\tLoss: 0.0026\t|\tAcc: 27.4%\n",
      "\t(valid)\tLoss: 0.0024\t|\tAcc: 29.9%\n",
      "Epoch: 15\n",
      "\t(train)\tLoss: 0.0026\t|\tAcc: 27.4%\n",
      "\t(valid)\tLoss: 0.0024\t|\tAcc: 30.0%\n",
      "Epoch: 16\n",
      "\t(train)\tLoss: 0.0026\t|\tAcc: 27.5%\n",
      "\t(valid)\tLoss: 0.0024\t|\tAcc: 30.1%\n",
      "Epoch: 17\n",
      "\t(train)\tLoss: 0.0026\t|\tAcc: 27.5%\n",
      "\t(valid)\tLoss: 0.0024\t|\tAcc: 30.1%\n",
      "Epoch: 18\n",
      "\t(train)\tLoss: 0.0026\t|\tAcc: 27.6%\n",
      "\t(valid)\tLoss: 0.0024\t|\tAcc: 30.1%\n",
      "Epoch: 19\n",
      "\t(train)\tLoss: 0.0025\t|\tAcc: 27.7%\n",
      "\t(valid)\tLoss: 0.0024\t|\tAcc: 30.2%\n",
      "Ended: 2021-03-12 21:04:53.319427\n",
      "Runtime: 0:05:39.812084\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "print(f'Started: {start_time}')\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train_classification(train_dataset,\n",
    "                                                 model=model,\n",
    "                                                 criterion=criterion, \n",
    "                                                 optimizer=optimizer,\n",
    "                                                 batch_size=BATCH_SIZE,\n",
    "                                                 device=device,\n",
    "                                                 scheduler=scheduler)\n",
    "    valid_loss, valid_acc = test_classification(val_dataset,\n",
    "                                                model=model,\n",
    "                                                criterion=criterion, \n",
    "                                                batch_size=BATCH_SIZE, \n",
    "                                                device=device)\n",
    "\n",
    "    print(f'Epoch: {epoch}')\n",
    "    print(f'\\t(train)\\tLoss: {train_loss:.4f}\\t|\\tAcc: {train_acc * 100:.1f}%')\n",
    "    print(f'\\t(valid)\\tLoss: {valid_loss:.4f}\\t|\\tAcc: {valid_acc * 100:.1f}%')\n",
    "\n",
    "end_time = datetime.now()\n",
    "runtime = end_time - start_time\n",
    "print(f'Ended: {end_time}')\n",
    "print(f'Runtime: {runtime}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([25, 18,  9,  ..., 65, 47, 25], device='cuda:0')"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(test_dataset.X_tensor.to(device)).argmax(1)\n",
    "preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classification report"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Roger\\.conda\\envs\\adsi_ass_2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     precision    recall  f1-score   support\n",
      "\n",
      "                            Altbier       0.38      0.33      0.35      1521\n",
      "             American Adjunct Lager       0.55      0.75      0.63      6085\n",
      "           American Amber / Red Ale       0.19      0.24      0.21      9288\n",
      "         American Amber / Red Lager       0.27      0.34      0.30      1887\n",
      "                American Barleywine       0.23      0.01      0.02      5390\n",
      "                 American Black Ale       0.41      0.06      0.10      2394\n",
      "                American Blonde Ale       0.26      0.02      0.03      2594\n",
      "                 American Brown Ale       0.27      0.09      0.14      5066\n",
      "            American Dark Wheat Ale       0.00      0.00      0.00       296\n",
      "     American Double / Imperial IPA       0.28      0.34      0.31     17159\n",
      " American Double / Imperial Pilsner       0.06      0.00      0.01      1109\n",
      "   American Double / Imperial Stout       0.36      0.46      0.40     10187\n",
      "                       American IPA       0.19      0.53      0.28     23356\n",
      "               American Malt Liquor       0.76      0.21      0.33       759\n",
      "            American Pale Ale (APA)       0.18      0.18      0.18     12479\n",
      "                American Pale Lager       0.47      0.22      0.30      1871\n",
      "            American Pale Wheat Ale       0.19      0.17      0.18      4900\n",
      "                    American Porter       0.23      0.22      0.22     10097\n",
      "                     American Stout       0.21      0.29      0.25      4966\n",
      "                American Strong Ale       0.25      0.41      0.31      6335\n",
      "                  American Wild Ale       0.29      0.38      0.33      3494\n",
      "                      Baltic Porter       0.43      0.43      0.43      2322\n",
      "                   Belgian Dark Ale       0.00      0.00      0.00      1278\n",
      "                        Belgian IPA       0.41      0.15      0.22      2428\n",
      "                   Belgian Pale Ale       0.63      0.24      0.34      3954\n",
      "            Belgian Strong Dark Ale       0.39      0.39      0.39      7511\n",
      "            Belgian Strong Pale Ale       0.39      0.29      0.34      6181\n",
      "                 Berliner Weissbier       0.38      0.22      0.28       712\n",
      "    Bière de Champagne / Bière Brut       0.33      0.12      0.18       211\n",
      "                     Bière de Garde       0.50      0.37      0.42      1340\n",
      "                        Black & Tan       0.99      0.15      0.26       459\n",
      "                               Bock       0.11      0.30      0.16      2323\n",
      "                            Braggot       0.17      0.09      0.11       207\n",
      "     California Common / Steam Beer       0.19      0.31      0.24       809\n",
      "                         Chile Beer       0.95      0.16      0.27       496\n",
      "                          Cream Ale       0.24      0.14      0.18      1052\n",
      "                     Czech Pilsener       0.67      0.34      0.45      2484\n",
      "                         Doppelbock       0.43      0.33      0.37      4380\n",
      "          Dortmunder / Export Lager       0.54      0.14      0.22       928\n",
      "                             Dubbel       0.37      0.09      0.15      4036\n",
      "                       Dunkelweizen       0.21      0.00      0.01      1426\n",
      "                            Eisbock       0.22      0.32      0.26       506\n",
      "                 English Barleywine       0.57      0.16      0.25      2798\n",
      "                     English Bitter       0.26      0.16      0.20      1783\n",
      "                  English Brown Ale       0.36      0.24      0.29      3870\n",
      "              English Dark Mild Ale       0.75      0.05      0.09       482\n",
      "       English India Pale Ale (IPA)       0.22      0.11      0.14      3218\n",
      "                   English Pale Ale       0.37      0.51      0.43      4631\n",
      "              English Pale Mild Ale       0.00      0.00      0.00       135\n",
      "                     English Porter       0.38      0.22      0.28      2225\n",
      "                      English Stout       0.38      0.07      0.12       605\n",
      "                 English Strong Ale       0.34      0.15      0.21       982\n",
      "                    Euro Dark Lager       0.29      0.10      0.15       916\n",
      "                    Euro Pale Lager       0.57      0.73      0.64      3673\n",
      "                  Euro Strong Lager       0.55      0.18      0.27       542\n",
      "Extra Special / Strong Bitter (ESB)       0.25      0.08      0.12      3539\n",
      "                               Faro       0.00      0.00      0.00       117\n",
      "                 Flanders Oud Bruin       0.42      0.27      0.33       944\n",
      "                   Flanders Red Ale       0.86      0.60      0.71      1332\n",
      "             Foreign / Export Stout       0.41      0.40      0.41      1185\n",
      "             Fruit / Vegetable Beer       0.27      0.35      0.30      6710\n",
      "                    German Pilsener       0.44      0.27      0.34      4416\n",
      "                               Gose       0.58      0.60      0.59       119\n",
      "                             Gueuze       0.67      0.35      0.46      1206\n",
      "                           Happoshu       0.00      0.00      0.00        46\n",
      "                         Hefeweizen       0.36      0.29      0.32      5675\n",
      "               Herbed / Spiced Beer       0.43      0.04      0.08      2081\n",
      "                    Irish Dry Stout       0.64      0.52      0.57      2537\n",
      "                      Irish Red Ale       0.66      0.12      0.20      1572\n",
      "                Japanese Rice Lager       0.64      0.88      0.74       308\n",
      "         Keller Bier / Zwickel Bier       0.42      0.10      0.17       525\n",
      "                      Kristalweizen       0.20      0.04      0.07       426\n",
      "                              Kvass       0.00      0.00      0.00        63\n",
      "                             Kölsch       0.57      0.17      0.26      1678\n",
      "                     Lambic - Fruit       0.54      0.75      0.63      2152\n",
      "                 Lambic - Unblended       0.00      0.00      0.00       227\n",
      "                        Light Lager       0.49      0.48      0.49      2759\n",
      "                   Low Alcohol Beer       0.64      0.04      0.08       222\n",
      "              Maibock / Helles Bock       0.19      0.07      0.10      2087\n",
      "                 Milk / Sweet Stout       0.31      0.47      0.37      2623\n",
      "                Munich Dunkel Lager       0.34      0.21      0.26      1566\n",
      "                Munich Helles Lager       0.40      0.15      0.22      1600\n",
      "               Märzen / Oktoberfest       0.30      0.11      0.17      4777\n",
      "                      Oatmeal Stout       0.25      0.22      0.24      3646\n",
      "                            Old Ale       0.47      0.25      0.32      2927\n",
      "                        Pumpkin Ale       0.35      0.13      0.18      3122\n",
      "                   Quadrupel (Quad)       0.38      0.51      0.43      3737\n",
      "                          Rauchbier       0.72      0.54      0.62       817\n",
      "                         Roggenbier       0.00      0.00      0.00       101\n",
      "             Russian Imperial Stout       0.31      0.41      0.35     10792\n",
      "                           Rye Beer       0.29      0.18      0.22      2054\n",
      "                              Sahti       0.91      0.20      0.32       198\n",
      "             Saison / Farmhouse Ale       0.38      0.43      0.40      6327\n",
      "                        Schwarzbier       0.42      0.28      0.33      1956\n",
      "             Scotch Ale / Wee Heavy       0.41      0.18      0.25      3460\n",
      "                       Scottish Ale       0.41      0.29      0.34      1827\n",
      "Scottish Gruit / Ancient Herbed Ale       0.88      0.75      0.81       557\n",
      "                        Smoked Beer       0.00      0.00      0.00       556\n",
      "                             Tripel       0.30      0.20      0.24      6074\n",
      "                       Vienna Lager       0.21      0.04      0.07      1865\n",
      "                         Weizenbock       0.55      0.31      0.40      1922\n",
      "                          Wheatwine       0.00      0.00      0.00       731\n",
      "                      Winter Warmer       0.29      0.15      0.20      4130\n",
      "                            Witbier       0.35      0.18      0.24      5896\n",
      "\n",
      "                           accuracy                           0.30    317323\n",
      "                          macro avg       0.37      0.24      0.26    317323\n",
      "                       weighted avg       0.33      0.30      0.29    317323\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, le.inverse_transform(preds.cpu()))\n",
    "print(report)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save objects for production"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "path = models_dir / f'{artefact_prefix}_model'\n",
    "torch.save(model, path.with_suffix('.torch'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create pipe object\n",
    "\n",
    "This is for transforming the input prior to prediction."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "['D:\\\\git\\\\assignment_2\\\\models\\\\4_pytorch_pipe.sav']"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.concat([X_train, X_val, X_test])\n",
    "prod_pipe = create_preprocessing_pipe(X)\n",
    "\n",
    "path = models_dir / f'{artefact_prefix}_pipe'\n",
    "dump(prod_pipe, path.with_suffix('.sav'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save `LabelEncoder`\n",
    "\n",
    "This is required to get back the name of the name of the `beer_style`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "['D:\\\\git\\\\assignment_2\\\\models\\\\4_pytorch_label_encoder.sav']"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = models_dir / f'{artefact_prefix}_label_encoder'\n",
    "dump(le, path.with_suffix('.sav'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "adsi_ass_2",
   "language": "python",
   "display_name": "adsi_ass_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}