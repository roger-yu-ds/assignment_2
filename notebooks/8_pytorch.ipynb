{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook predicts the `beer_style` using a neural network on the PyTorch\n",
    "framework. It is a modification of the 5_pytorch.ipynb notebook. After 20\n",
    "epochs, there seems to be still some room for improvement.\n",
    "\n",
    "The same model is trained again for 60 more epochs.\n",
    "\n",
    "## Summary\n",
    "The increase of neurons has **not** improved the model performance. The\n",
    "[classification report](#Classification-report) shows that the validation\n",
    "accuracy increased to as high as 31.2%, and the test accuracy remains at 32%."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "artefact_prefix = '8_pytorch'\n",
    "target = 'beer_style'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "from dotenv import find_dotenv\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from category_encoders.binary import BinaryEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from joblib import dump, load\n",
    "\n",
    "from src.data.sets import merge_categories\n",
    "from src.data.sets import save_sets\n",
    "from src.data.sets import load_sets\n",
    "from src.data.sets import split_sets_random\n",
    "from src.data.sets import test_class_exclusion\n",
    "from src.models.performance import convert_cr_to_dataframe\n",
    "from src.models.pytorch import PytorchClassification_8\n",
    "from src.models.pytorch import get_device\n",
    "from src.models.pytorch import train_classification\n",
    "from src.models.pytorch import test_classification\n",
    "from src.models.pytorch import PytorchDataset\n",
    "from src.models.pipes import create_preprocessing_pipe\n",
    "from src.visualization.visualize import plot_confusion_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Set up directories"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "project_dir = Path(find_dotenv()).parent\n",
    "data_dir = project_dir / 'data'\n",
    "raw_data_dir = data_dir / 'raw'\n",
    "interim_data_dir = data_dir / 'interim'\n",
    "processed_data_dir = data_dir / 'processed'\n",
    "reports_dir = project_dir / 'reports'\n",
    "models_dir = project_dir / 'models'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Set directory\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "X_train, X_test, X_val, y_train, y_test, y_val = load_sets()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocess data\n",
    "\n",
    "1. The `brewery_name` is a feature with a very high cardinality, ~5700. One hot\n",
    " encoding is not feasible as it will introduce 5700 very sparse columns.\n",
    " Another option is to use binary encoding, which would result in 14 new columns.\n",
    "1. Standard scaling is used to ensure that the binary columns ([0, 1])and the\n",
    "review columns ([1, 5]) are on the same scale."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('bin_encoder', BinaryEncoder(cols=['brewery_name'])),\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "X_train_trans = pipe.fit_transform(X_train)\n",
    "X_val_trans = pipe.transform(X_val)\n",
    "X_test_trans = pipe.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "(951968, 18)"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_trans.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "18"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = X_train_trans.shape[1]\n",
    "n_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "104"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = y_train.nunique()\n",
    "n_classes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Encoding\n",
    "\n",
    "PyTorch accepts only numerical labels."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Roger\\.conda\\envs\\adsi_ass_2\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:251: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Roger\\.conda\\envs\\adsi_ass_2\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_train_trans = le.fit_transform(y_train.to_frame())\n",
    "y_val_trans = le.fit_transform(y_val.to_frame())\n",
    "y_test_trans = le.transform(y_test.to_frame())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "array([98, 89,  2, ..., 37, 94, 98])"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_trans"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convert to Pytorch tensors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda', index=0)"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_device()\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "train_dataset = PytorchDataset(X=X_train_trans, y=y_train_trans)\n",
    "val_dataset = PytorchDataset(X=X_val_trans, y=y_val_trans)\n",
    "test_dataset = PytorchDataset(X=X_test_trans, y=y_test_trans)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classification model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "PytorchClassification_8(\n  (layer_1): Linear(in_features=18, out_features=4096, bias=True)\n  (batchnorm1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n  (batchnorm2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layer_3): Linear(in_features=1024, out_features=256, bias=True)\n  (batchnorm3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layer_4): Linear(in_features=256, out_features=128, bias=True)\n  (batchnorm4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layer_5): Linear(in_features=128, out_features=64, bias=True)\n  (batchnorm5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layer_out): Linear(in_features=64, out_features=104, bias=True)\n  (relu): ReLU()\n  (dropout): Dropout(p=0.2, inplace=False)\n)"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PytorchClassification_8(n_features=n_features, n_classes=n_classes)\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "N_EPOCHS = 60\n",
    "BATCH_SIZE = 4096\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started: 2021-03-14 14:30:13.766633\n",
      "Epoch: 0\n",
      "\t(train)\tLoss: 0.0009\t|\tAcc: 16.6%\n",
      "\t(valid)\tLoss: 0.0008\t|\tAcc: 23.4%\n",
      "Epoch: 1\n",
      "\t(train)\tLoss: 0.0007\t|\tAcc: 23.6%\n",
      "\t(valid)\tLoss: 0.0007\t|\tAcc: 26.3%\n",
      "Epoch: 2\n",
      "\t(train)\tLoss: 0.0007\t|\tAcc: 25.2%\n",
      "\t(valid)\tLoss: 0.0007\t|\tAcc: 27.4%\n",
      "Epoch: 3\n",
      "\t(train)\tLoss: 0.0007\t|\tAcc: 25.9%\n",
      "\t(valid)\tLoss: 0.0007\t|\tAcc: 27.9%\n",
      "Epoch: 4\n",
      "\t(train)\tLoss: 0.0007\t|\tAcc: 26.3%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 28.3%\n",
      "Epoch: 5\n",
      "\t(train)\tLoss: 0.0007\t|\tAcc: 26.6%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 28.5%\n",
      "Epoch: 6\n",
      "\t(train)\tLoss: 0.0007\t|\tAcc: 26.9%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 28.8%\n",
      "Epoch: 7\n",
      "\t(train)\tLoss: 0.0007\t|\tAcc: 27.1%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 28.9%\n",
      "Epoch: 8\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 27.2%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 29.0%\n",
      "Epoch: 9\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 27.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 29.2%\n",
      "Epoch: 10\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 27.5%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 29.2%\n",
      "Epoch: 11\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 27.5%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 29.3%\n",
      "Epoch: 12\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 27.7%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 29.4%\n",
      "Epoch: 13\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 27.7%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 29.4%\n",
      "Epoch: 14\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 27.8%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 29.5%\n",
      "Epoch: 15\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 27.9%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 29.6%\n",
      "Epoch: 16\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 27.9%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 29.6%\n",
      "Epoch: 17\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 27.9%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 29.7%\n",
      "Epoch: 18\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.0%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 29.7%\n",
      "Epoch: 19\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.1%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 29.7%\n",
      "Epoch: 20\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.1%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 29.8%\n",
      "Epoch: 21\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.1%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 29.8%\n",
      "Epoch: 22\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.2%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 29.8%\n",
      "Epoch: 23\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.1%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 29.8%\n",
      "Epoch: 24\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.2%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 29.9%\n",
      "Epoch: 25\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.2%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 29.9%\n",
      "Epoch: 26\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.3%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 29.9%\n",
      "Epoch: 27\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.2%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 29.9%\n",
      "Epoch: 28\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.3%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 29.9%\n",
      "Epoch: 29\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.3%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 29.9%\n",
      "Epoch: 30\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.3%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 29.9%\n",
      "Epoch: 31\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.3%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 29.9%\n",
      "Epoch: 32\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.3%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 29.9%\n",
      "Epoch: 33\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.3%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 29.9%\n",
      "Epoch: 34\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.3%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 35\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.3%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 29.9%\n",
      "Epoch: 36\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 29.9%\n",
      "Epoch: 37\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.3%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 38\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 39\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 40\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 41\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 42\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.3%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 43\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.3%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 44\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.3%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 45\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 46\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 47\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 48\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 49\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 50\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 51\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.3%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 52\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 53\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 54\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 55\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 56\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 57\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 58\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 59\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Ended: 2021-03-14 14:42:40.984752\n",
      "Runtime: 0:12:27.218119\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "print(f'Started: {start_time}')\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train_classification(train_dataset,\n",
    "                                                 model=model,\n",
    "                                                 criterion=criterion, \n",
    "                                                 optimizer=optimizer,\n",
    "                                                 batch_size=BATCH_SIZE,\n",
    "                                                 device=device,\n",
    "                                                 scheduler=scheduler)\n",
    "    valid_loss, valid_acc = test_classification(val_dataset,\n",
    "                                                model=model,\n",
    "                                                criterion=criterion, \n",
    "                                                batch_size=BATCH_SIZE, \n",
    "                                                device=device)\n",
    "\n",
    "    print(f'Epoch: {epoch}')\n",
    "    print(f'\\t(train)\\tLoss: {train_loss:.4f}\\t|\\tAcc: {train_acc * 100:.1f}%')\n",
    "    print(f'\\t(valid)\\tLoss: {valid_loss:.4f}\\t|\\tAcc: {valid_acc * 100:.1f}%')\n",
    "\n",
    "end_time = datetime.now()\n",
    "runtime = end_time - start_time\n",
    "print(f'Ended: {end_time}')\n",
    "print(f'Runtime: {runtime}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N_EPOCHS = 20\n",
    "BATCH_SIZE = 4096\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started: 2021-03-14 14:45:36.016408\n",
      "Epoch: 0\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 1\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 2\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.5%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 3\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 4\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 5\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.5%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 6\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 7\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 8\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 9\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 10\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.5%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 11\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.5%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 12\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 13\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 14\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 15\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 16\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 17\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 18\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Epoch: 19\n",
      "\t(train)\tLoss: 0.0006\t|\tAcc: 28.4%\n",
      "\t(valid)\tLoss: 0.0006\t|\tAcc: 30.0%\n",
      "Ended: 2021-03-14 14:49:46.324103\n",
      "Runtime: 0:04:10.307695\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "print(f'Started: {start_time}')\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train_classification(train_dataset,\n",
    "                                                 model=model,\n",
    "                                                 criterion=criterion,\n",
    "                                                 optimizer=optimizer,\n",
    "                                                 batch_size=BATCH_SIZE,\n",
    "                                                 device=device,\n",
    "                                                 scheduler=scheduler)\n",
    "    valid_loss, valid_acc = test_classification(val_dataset,\n",
    "                                                model=model,\n",
    "                                                criterion=criterion,\n",
    "                                                batch_size=BATCH_SIZE,\n",
    "                                                device=device)\n",
    "\n",
    "    print(f'Epoch: {epoch}')\n",
    "    print(f'\\t(train)\\tLoss: {train_loss:.4f}\\t|\\tAcc: {train_acc * 100:.1f}%')\n",
    "    print(f'\\t(valid)\\tLoss: {valid_loss:.4f}\\t|\\tAcc: {valid_acc * 100:.1f}%')\n",
    "\n",
    "end_time = datetime.now()\n",
    "runtime = end_time - start_time\n",
    "print(f'Ended: {end_time}')\n",
    "print(f'Runtime: {runtime}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "PytorchClassification_8(\n  (layer_1): Linear(in_features=18, out_features=256, bias=True)\n  (batchnorm1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layer_2): Linear(in_features=256, out_features=128, bias=True)\n  (batchnorm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layer_out): Linear(in_features=128, out_features=104, bias=True)\n  (relu): ReLU()\n  (dropout): Dropout(p=0.2, inplace=False)\n)"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the CPU version if the GPU runs out of memory.\n",
    "# preds = model(test_dataset.X_tensor.to(device)).argmax(1)\n",
    "model.to('cpu')\n",
    "preds = model(test_dataset.X_tensor).argmax(1)\n",
    "preds\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classification report"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Roger\\.conda\\envs\\adsi_ass_2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     precision    recall  f1-score   support\n",
      "\n",
      "                            Altbier       0.34      0.36      0.35      1521\n",
      "             American Adjunct Lager       0.55      0.73      0.63      6085\n",
      "           American Amber / Red Ale       0.19      0.22      0.20      9288\n",
      "         American Amber / Red Lager       0.32      0.33      0.32      1887\n",
      "                American Barleywine       0.22      0.03      0.06      5390\n",
      "                 American Black Ale       0.36      0.06      0.10      2394\n",
      "                American Blonde Ale       0.20      0.04      0.06      2594\n",
      "                 American Brown Ale       0.25      0.11      0.15      5066\n",
      "            American Dark Wheat Ale       0.00      0.00      0.00       296\n",
      "     American Double / Imperial IPA       0.26      0.35      0.30     17159\n",
      " American Double / Imperial Pilsner       0.15      0.03      0.05      1109\n",
      "   American Double / Imperial Stout       0.35      0.47      0.40     10187\n",
      "                       American IPA       0.20      0.46      0.28     23356\n",
      "               American Malt Liquor       0.68      0.23      0.35       759\n",
      "            American Pale Ale (APA)       0.17      0.19      0.18     12479\n",
      "                American Pale Lager       0.42      0.22      0.29      1871\n",
      "            American Pale Wheat Ale       0.19      0.16      0.17      4900\n",
      "                    American Porter       0.22      0.23      0.22     10097\n",
      "                     American Stout       0.21      0.30      0.25      4966\n",
      "                American Strong Ale       0.25      0.42      0.31      6335\n",
      "                  American Wild Ale       0.30      0.39      0.33      3494\n",
      "                      Baltic Porter       0.43      0.43      0.43      2322\n",
      "                   Belgian Dark Ale       0.15      0.00      0.01      1278\n",
      "                        Belgian IPA       0.39      0.16      0.23      2428\n",
      "                   Belgian Pale Ale       0.48      0.25      0.33      3954\n",
      "            Belgian Strong Dark Ale       0.38      0.39      0.38      7511\n",
      "            Belgian Strong Pale Ale       0.43      0.28      0.34      6181\n",
      "                 Berliner Weissbier       0.62      0.19      0.29       712\n",
      "    Bière de Champagne / Bière Brut       0.33      0.05      0.09       211\n",
      "                     Bière de Garde       0.50      0.39      0.44      1340\n",
      "                        Black & Tan       0.32      0.17      0.22       459\n",
      "                               Bock       0.11      0.30      0.17      2323\n",
      "                            Braggot       0.00      0.00      0.00       207\n",
      "     California Common / Steam Beer       0.24      0.22      0.23       809\n",
      "                         Chile Beer       0.93      0.16      0.28       496\n",
      "                          Cream Ale       0.21      0.12      0.15      1052\n",
      "                     Czech Pilsener       0.64      0.33      0.43      2484\n",
      "                         Doppelbock       0.40      0.32      0.36      4380\n",
      "          Dortmunder / Export Lager       0.28      0.18      0.22       928\n",
      "                             Dubbel       0.30      0.18      0.23      4036\n",
      "                       Dunkelweizen       0.26      0.00      0.01      1426\n",
      "                            Eisbock       0.30      0.18      0.22       506\n",
      "                 English Barleywine       0.43      0.18      0.26      2798\n",
      "                     English Bitter       0.20      0.16      0.18      1783\n",
      "                  English Brown Ale       0.37      0.26      0.30      3870\n",
      "              English Dark Mild Ale       0.46      0.09      0.14       482\n",
      "       English India Pale Ale (IPA)       0.23      0.13      0.17      3218\n",
      "                   English Pale Ale       0.39      0.48      0.43      4631\n",
      "              English Pale Mild Ale       0.00      0.00      0.00       135\n",
      "                     English Porter       0.36      0.25      0.30      2225\n",
      "                      English Stout       0.29      0.06      0.10       605\n",
      "                 English Strong Ale       0.35      0.13      0.18       982\n",
      "                    Euro Dark Lager       0.25      0.15      0.19       916\n",
      "                    Euro Pale Lager       0.53      0.70      0.60      3673\n",
      "                  Euro Strong Lager       0.47      0.21      0.29       542\n",
      "Extra Special / Strong Bitter (ESB)       0.23      0.08      0.12      3539\n",
      "                               Faro       0.00      0.00      0.00       117\n",
      "                 Flanders Oud Bruin       0.45      0.23      0.30       944\n",
      "                   Flanders Red Ale       0.78      0.63      0.70      1332\n",
      "             Foreign / Export Stout       0.42      0.38      0.40      1185\n",
      "             Fruit / Vegetable Beer       0.26      0.37      0.31      6710\n",
      "                    German Pilsener       0.31      0.32      0.31      4416\n",
      "                               Gose       0.59      0.60      0.59       119\n",
      "                             Gueuze       0.65      0.37      0.47      1206\n",
      "                           Happoshu       1.00      0.07      0.12        46\n",
      "                         Hefeweizen       0.34      0.29      0.31      5675\n",
      "               Herbed / Spiced Beer       0.25      0.06      0.10      2081\n",
      "                    Irish Dry Stout       0.60      0.53      0.56      2537\n",
      "                      Irish Red Ale       0.47      0.12      0.19      1572\n",
      "                Japanese Rice Lager       0.66      0.87      0.75       308\n",
      "         Keller Bier / Zwickel Bier       0.33      0.20      0.25       525\n",
      "                      Kristalweizen       0.20      0.04      0.07       426\n",
      "                              Kvass       0.56      0.32      0.40        63\n",
      "                             Kölsch       0.39      0.20      0.27      1678\n",
      "                     Lambic - Fruit       0.55      0.74      0.63      2152\n",
      "                 Lambic - Unblended       0.68      0.11      0.19       227\n",
      "                        Light Lager       0.44      0.49      0.47      2759\n",
      "                   Low Alcohol Beer       0.46      0.06      0.10       222\n",
      "              Maibock / Helles Bock       0.18      0.10      0.12      2087\n",
      "                 Milk / Sweet Stout       0.31      0.47      0.38      2623\n",
      "                Munich Dunkel Lager       0.35      0.13      0.19      1566\n",
      "                Munich Helles Lager       0.41      0.13      0.20      1600\n",
      "               Märzen / Oktoberfest       0.28      0.13      0.18      4777\n",
      "                      Oatmeal Stout       0.26      0.24      0.25      3646\n",
      "                            Old Ale       0.43      0.27      0.33      2927\n",
      "                        Pumpkin Ale       0.33      0.14      0.19      3122\n",
      "                   Quadrupel (Quad)       0.43      0.44      0.44      3737\n",
      "                          Rauchbier       0.67      0.55      0.60       817\n",
      "                         Roggenbier       0.48      0.12      0.19       101\n",
      "             Russian Imperial Stout       0.32      0.42      0.36     10792\n",
      "                           Rye Beer       0.29      0.18      0.22      2054\n",
      "                              Sahti       0.92      0.17      0.29       198\n",
      "             Saison / Farmhouse Ale       0.36      0.42      0.39      6327\n",
      "                        Schwarzbier       0.37      0.28      0.32      1956\n",
      "             Scotch Ale / Wee Heavy       0.32      0.22      0.26      3460\n",
      "                       Scottish Ale       0.37      0.28      0.32      1827\n",
      "Scottish Gruit / Ancient Herbed Ale       0.88      0.75      0.81       557\n",
      "                        Smoked Beer       0.20      0.02      0.03       556\n",
      "                             Tripel       0.29      0.20      0.24      6074\n",
      "                       Vienna Lager       0.16      0.06      0.09      1865\n",
      "                         Weizenbock       0.56      0.30      0.39      1922\n",
      "                          Wheatwine       0.00      0.00      0.00       731\n",
      "                      Winter Warmer       0.29      0.17      0.22      4130\n",
      "                            Witbier       0.32      0.20      0.24      5896\n",
      "\n",
      "                           accuracy                           0.30    317323\n",
      "                          macro avg       0.37      0.25      0.27    317323\n",
      "                       weighted avg       0.32      0.30      0.29    317323\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, le.inverse_transform(preds.cpu()))\n",
    "print(report)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save objects for production"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "path = models_dir / f'{artefact_prefix}_model'\n",
    "torch.save(model, path.with_suffix('.torch'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create pipe object\n",
    "\n",
    "This is for transforming the input prior to prediction."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "['D:\\\\git\\\\assignment_2\\\\models\\\\9_pytorch_pipe.sav']"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.concat([X_train, X_val, X_test])\n",
    "prod_pipe = create_preprocessing_pipe(X)\n",
    "\n",
    "path = models_dir / f'{artefact_prefix}_pipe'\n",
    "dump(prod_pipe, path.with_suffix('.sav'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save `LabelEncoder`\n",
    "\n",
    "This is required to get back the name of the name of the `beer_style`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "['D:\\\\git\\\\assignment_2\\\\models\\\\9_pytorch_label_encoder.sav']"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = models_dir / f'{artefact_prefix}_label_encoder'\n",
    "dump(le, path.with_suffix('.sav'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "adsi_ass_2",
   "language": "python",
   "display_name": "adsi_ass_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}